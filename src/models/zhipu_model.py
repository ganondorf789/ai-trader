"""
ğŸŒ™ Moon Dev's ZhipuAI (æ™ºè°±AI) Model Implementation
Built with love by Moon Dev ğŸš€

Supports GLM-4 series models via zhipuai SDK
"""

from zhipuai import ZhipuAI
from termcolor import cprint
from .base_model import BaseModel, ModelResponse


class ZhipuModel(BaseModel):
    """Implementation for ZhipuAI's GLM models (æ™ºè°±AI)"""

    AVAILABLE_MODELS = {
        "glm-4-plus": {
            "description": "GLM-4 Plus - é«˜æ™ºèƒ½æ——èˆ°æ¨¡å‹ï¼Œæ€§èƒ½å…¨é¢æå‡",
            "input_price": "Â¥0.05/1K tokens",
            "output_price": "Â¥0.05/1K tokens",
            "context_length": 128000
        },
        "glm-4-0520": {
            "description": "GLM-4 (0520) - é«˜æ™ºèƒ½æ¨¡å‹ï¼Œé€‚åˆå¤æ‚ä»»åŠ¡",
            "input_price": "Â¥0.1/1K tokens",
            "output_price": "Â¥0.1/1K tokens",
            "context_length": 128000
        },
        "glm-4-air": {
            "description": "GLM-4 Air - é«˜æ€§ä»·æ¯”æ¨¡å‹ï¼Œæ¨ç†å¿«é€Ÿ",
            "input_price": "Â¥0.001/1K tokens",
            "output_price": "Â¥0.001/1K tokens",
            "context_length": 128000
        },
        "glm-4-airx": {
            "description": "GLM-4 AirX - æé€Ÿæ¨ç†ï¼Œé€‚åˆå®æ—¶åœºæ™¯",
            "input_price": "Â¥0.01/1K tokens",
            "output_price": "Â¥0.01/1K tokens",
            "context_length": 8192
        },
        "glm-4-flash": {
            "description": "GLM-4 Flash - å…è´¹æ¨¡å‹ï¼Œé€‚åˆè½»é‡ä»»åŠ¡",
            "input_price": "Free",
            "output_price": "Free",
            "context_length": 128000
        },
        "glm-4-flashx": {
            "description": "GLM-4 FlashX - å…è´¹æé€Ÿæ¨¡å‹",
            "input_price": "Free",
            "output_price": "Free",
            "context_length": 128000
        },
        "glm-4-long": {
            "description": "GLM-4 Long - è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒ1M tokens",
            "input_price": "Â¥0.001/1K tokens",
            "output_price": "Â¥0.001/1K tokens",
            "context_length": 1000000
        },
        "glm-4v-plus": {
            "description": "GLM-4V Plus - å¤šæ¨¡æ€æ——èˆ°ï¼Œæ”¯æŒå›¾åƒç†è§£",
            "input_price": "Â¥0.01/1K tokens",
            "output_price": "Â¥0.01/1K tokens",
            "context_length": 8192
        },
        "glm-4v": {
            "description": "GLM-4V - å¤šæ¨¡æ€æ¨¡å‹ï¼Œæ”¯æŒå›¾åƒç†è§£",
            "input_price": "Â¥0.05/1K tokens",
            "output_price": "Â¥0.05/1K tokens",
            "context_length": 2048
        }
    }

    def __init__(self, api_key: str, model_name: str = "glm-4-flash", **kwargs):
        self.model_name = model_name
        self.max_tokens = kwargs.get("max_tokens", 4096)
        super().__init__(api_key, **kwargs)

    def initialize_client(self, **kwargs) -> None:
        """Initialize the ZhipuAI client"""
        try:
            self.client = ZhipuAI(api_key=self.api_key)
            cprint(f"âœ¨ Moon Dev's magic initialized ZhipuAI model: {self.model_name} ğŸŒŸ", "green")
        except Exception as e:
            cprint(f"âŒ Failed to initialize ZhipuAI model: {str(e)}", "red")
            self.client = None

    def generate_response(self, system_prompt, user_content, **kwargs):
        """Generate a response using the ZhipuAI model"""
        try:
            temperature = kwargs.get("temperature", 0.7)
            max_tokens = kwargs.get("max_tokens", self.max_tokens)

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content}
            ]

            cprint(f"ğŸ¤” Moon Dev's {self.model_name} is thinking...", "yellow")

            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )

            content = response.choices[0].message.content

            return ModelResponse(
                content=content.strip() if content else "",
                raw_response=response,
                model_name=self.model_name,
                usage=response.usage.model_dump() if hasattr(response, 'usage') and response.usage else None
            )

        except Exception as e:
            cprint(f"âŒ ZhipuAI generation error: {repr(e)}", "red")
            raise

    def is_available(self) -> bool:
        """Check if ZhipuAI is available"""
        return self.client is not None

    @property
    def model_type(self) -> str:
        return "zhipu"
